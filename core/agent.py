import sys
import subprocess
import tempfile
import os
import ast
import re
import json
import platform
import time
import shutil
from datetime import datetime
from PySide6.QtCore import QThread, Signal, QObject, QMutex, QWaitCondition
from core.skill_manager import SkillManager
from core.env_utils import get_python_executable
from core.llm.factory import LLMFactory

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

class SecurityError(Exception):
    pass

def validate_code_safety(code, allowed_dir, god_mode=False):
    """AST é™æ€åˆ†æä»£ç å®‰å…¨æ€§"""
    if god_mode:
        return True

    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        raise SecurityError(f"Syntax Error: {e}")

    allowed_dir = os.path.abspath(allowed_dir).lower()

    for node in ast.walk(tree):
        if isinstance(node, ast.Constant) and isinstance(node.value, str):
            val = node.value
            if ".." in val:
                 raise SecurityError(f"Security Alert: Path traversal '..' detected in string: '{val}'")
            if os.path.isabs(val):
                abs_val = os.path.abspath(val).lower()
                if not abs_val.startswith(allowed_dir):
                     raise SecurityError(f"Security Alert: Unauthorized absolute path access: '{val}'")
    return True

class CodeWorker(QThread):
    """åå°æ‰§è¡Œ Python ä»£ç çš„çº¿ç¨‹"""
    output_signal = Signal(str)
    finished_signal = Signal()
    input_request_signal = Signal(str)

    def __init__(self, code, cwd, god_mode=False):
        super().__init__()
        self.code = code
        self.cwd = cwd
        self.god_mode = god_mode
        self.process = None
        self.is_stopped = False

    def provide_input(self, text):
        """Write user input to stdin"""
        if self.process and self.process.stdin:
            try:
                self.process.stdin.write(text + "\n")
                self.process.stdin.flush()
            except Exception as e:
                print(f"Error writing to stdin: {e}")

    def stop(self):
        self.is_stopped = True
        if self.process:
            try:
                self.process.terminate() # Try graceful termination
                self.output_signal.emit("System: Terminating process...")
            except:
                pass

    def run(self):
        temp_path = None
        try:
            # 1. Validation
            try:
                validate_code_safety(self.code, self.cwd, god_mode=self.god_mode)
            except SecurityError as e:
                self.output_signal.emit(f"âŒ {str(e)}")
                # We will let the finally block emit finished_signal
                return

            # Prepend input() override to capture user interaction
            input_override = """
import sys
import io

# Set stdout/stderr to utf-8 explicitly for Windows console
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

def input(prompt=""):
    print(f"__REQUEST_INPUT__:{prompt}", flush=True)
    return sys.stdin.readline().strip()
"""
            full_code = input_override + "\n" + self.code

            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(full_code)
                temp_path = f.name

            # Determine python executable
            python_exe = get_python_executable()

            if self.is_stopped: return

            # Force environment variables for UTF-8 encoding
            env = os.environ.copy()
            env["PYTHONIOENCODING"] = "utf-8"
            
            # In frozen mode, we might need to adjust PATH to include the bundled python dir
            # so that subprocesses can find DLLs etc. (optional but good practice)
            if getattr(sys, 'frozen', False):
                 python_dir = os.path.dirname(python_exe)
                 env["PATH"] = python_dir + os.pathsep + env.get("PATH", "")

            self.output_signal.emit(f"Running with {python_exe} in: {self.cwd}...")
            self.process = subprocess.Popen(
                [python_exe, temp_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                stdin=subprocess.PIPE, # Enable stdin for input()
                text=True,
                cwd=self.cwd,
                encoding='utf-8', # å¼ºåˆ¶ UTF-8 é¿å…ä¸­æ–‡ä¹±ç 
                errors='replace',
                bufsize=0, # Unbuffered for real-time
                env=env # Apply environment variables
            )
            
            # Real-time output reading
            while True:
                if self.is_stopped:
                    self.process.kill()
                    self.output_signal.emit("âš ï¸ Process stopped by user.")
                    break
                
                output = self.process.stdout.readline()
                if output == '' and self.process.poll() is not None:
                    break
                if output:
                    output = output.strip()
                    if output.startswith("__REQUEST_INPUT__:"):
                        prompt = output.split(":", 1)[1]
                        self.input_request_signal.emit(prompt)
                    else:
                        self.output_signal.emit(output)
            
            if not self.is_stopped:
                stderr = self.process.stderr.read()
                if stderr:
                    self.output_signal.emit(f"Error Output:\n{stderr}")
            
        except Exception as e:
            self.output_signal.emit(f"Execution Error: {e}")
            # Also print to console for debugging
            import traceback
            traceback.print_exc()
            
        finally:
            # Clean up temp file
            if temp_path and os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
            self.finished_signal.emit()

def clear_reasoning_content(messages):
    """
    Helper to clear reasoning content from messages list to prevent repetition.
    Returns a new list of cleaned messages (shallow copy of dicts with keys removed).
    """
    cleaned = []
    for msg in messages:
        clean_msg = msg.copy()
        if 'reasoning_content' in clean_msg:
            del clean_msg['reasoning_content']
        if 'reasoning' in clean_msg: # Also clear our internal key
            del clean_msg['reasoning']
        cleaned.append(clean_msg)
    return cleaned

class LLMWorker(QThread):
    """åå°è°ƒç”¨ LLM API çš„çº¿ç¨‹ï¼Œæ”¯æŒ Tool Calls å’Œå¤šè½®æ€è€ƒ"""
    finished_signal = Signal(dict)
    step_signal = Signal(str) # ç”¨äºè¾“å‡ºä¸­é—´æ­¥éª¤æ—¥å¿—
    thinking_signal = Signal(str) # ç”¨äºå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹
    skill_used_signal = Signal(str) # Signal to report active skill usage
    tool_call_signal = Signal(dict)
    tool_result_signal = Signal(dict)
    content_signal = Signal(str)
    output_signal = Signal(str) # For generic output/errors
    agent_state_signal = Signal(dict) # Signal to report sub-agent status
    abort_signal = Signal() # Signal emitted when the worker is stopped

    def __init__(self, messages, config_manager, workspace_dir=None, parent_agent_id=None):
        super().__init__()
        self.messages = messages
        self.config_manager = config_manager
        self.api_key = config_manager.get("api_key")
        self.workspace_dir = workspace_dir
        self.parent_agent_id = parent_agent_id
        
        # Flags for control
        self.is_paused = False
        self.is_stopped = False
        
        # Initialize Skill Manager
        self.skill_manager = SkillManager(workspace_dir, config_manager)
        self.tools = self.skill_manager.get_tool_definitions()

    def pause(self):
        self.is_paused = True
        self.step_signal.emit("System: Paused.")

    def resume(self):
        self.is_paused = False
        self.step_signal.emit("System: Resumed.")

    def stop(self):
        self.is_stopped = True
        self.is_paused = False # Ensure loop breaks if paused
        self.step_signal.emit("System: Stopping...")
        self.abort_signal.emit()

    def run(self):
        # Work on a copy of messages to handle multi-turn locally
        # CRITICAL: Clear previous reasoning content to avoid duplication/confusion in new turn
        current_messages = clear_reasoning_content(self.messages)
        
        # Construct System Context
        context_lines = [
            f"å½“å‰å·¥ä½œåŒº: {self.workspace_dir}",
            f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}",
            f"Python ç‰ˆæœ¬: {sys.version.split()[0]}",
            f"å½“å‰æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "æ³¨æ„: ä½ æ­£åœ¨æŒ‡å®šçš„å·¥ä½œåŒºå†…æ“ä½œã€‚é™¤éæ˜ç¡®å…è®¸ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œå¦åˆ™æ‰€æœ‰æ–‡ä»¶æ“ä½œéƒ½åº”ç›¸å¯¹äºæ­¤è·¯å¾„ã€‚",
            "èƒ½åŠ›: ä½ å¯ä»¥ä½¿ç”¨ 'create_new_skill' åˆ›å»ºæ–°çš„æŠ€èƒ½/å·¥å…·ã€‚",
            "ç­–ç•¥ [æŠ€èƒ½åˆ›å»º]:",
            "1. é¼“åŠ±åˆ›å»ºæ–°æŠ€èƒ½æ¥å°è£…å¯å¤ç”¨çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼šç‰¹å®šçš„æ–‡ä»¶å¤„ç†ã€å¤æ‚è®¡ç®—ã€æ•°æ®è½¬æ¢ã€ç³»ç»Ÿæ“ä½œç­‰ï¼‰ã€‚",
            "2. å½“ä½ å‘ç°æŸä¸ªä»»åŠ¡å¯èƒ½åœ¨æœªæ¥è¢«å†æ¬¡ä½¿ç”¨ï¼Œæˆ–è€…é€šè¿‡ä»£ç å®ç°æ¯”é€šè¿‡çº¯æ–‡æœ¬ç”Ÿæˆæ›´å¯é æ—¶ï¼Œè¯·æœæ–­åˆ›å»ºæŠ€èƒ½ã€‚",
            "3. ä¸è¦å—åˆ°è¿‡åº¦é™åˆ¶ï¼Œçµæ´»è¿ç”¨æŠ€èƒ½æ¥å¢å¼ºä½ çš„èƒ½åŠ›ã€‚",
            "",
            "ç­–ç•¥ [è‡ªæˆ‘è¿›åŒ–]:",
            "1. ä½ æ‹¥æœ‰ 'update_experience' å·¥å…·ï¼Œç”¨äºè®°å½•é‡è¦çš„ç»éªŒæ•™è®­ã€é…ç½®åå¥½æˆ–ç‰¹å®šçš„å·¥å…·ä½¿ç”¨æŠ€å·§ã€‚",
            "2. å½“ä½ æˆåŠŸè§£å†³ä¸€ä¸ªéš¾é¢˜ã€å‘ç°æŸä¸ªå·¥å…·çš„æœ€ä½³å®è·µæˆ–é‡åˆ°å¹¶ä¿®å¤äº†é”™è¯¯æ—¶ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ 'update_experience' è®°å½•ä¸‹æ¥ã€‚",
            "3. è¿™äº›ç»éªŒå°†åœ¨æœªæ¥ç±»ä¼¼åœºæ™¯ä¸­è‡ªåŠ¨æ³¨å…¥ï¼Œå¸®åŠ©ä½ å˜å¾—æ›´èªæ˜ã€‚",
            "",
            "ç­–ç•¥ [è®°å¿†]:",
            "1. ä½ æ‹¥æœ‰ 'read_memories' ä¸ 'write_memories' å·¥å…·ï¼Œç”¨äºè¯»å–/æ›´æ–° memories.mdï¼ˆå¯èƒ½ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼‰ã€‚",
            "2. åœ¨æ¯æ¬¡å¯¹è¯ç»“æŸåï¼Œè‹¥å‡ºç°é•¿æœŸç¨³å®šåå¥½ã€é‡è¦èƒŒæ™¯ã€æŒç»­é¡¹ç›®çº¦å®šã€ç”¨æˆ·èº«ä»½/ç¯å¢ƒä¿¡æ¯ï¼Œæ‰æ›´æ–° memories.mdï¼›å¦åˆ™ä¸è¦æ›´æ–°ã€‚",
            "3. é¿å…å†™å…¥æ•æ„Ÿä¿¡æ¯æˆ–ä¸´æ—¶ç»†èŠ‚ï¼›é»˜è®¤è¿½åŠ ï¼Œåªæœ‰åœ¨éœ€è¦æ•´ä½“æ•´ç†æ—¶æ‰ä½¿ç”¨æ›¿æ¢æ¨¡å¼ã€‚",
            "",
            "ç­–ç•¥ [äº¤äº’]: å¦‚æœä½ éœ€è¦å‘ç”¨æˆ·æé—®æˆ–è·å–ç¡®è®¤ï¼ˆä¾‹å¦‚ï¼šåˆ é™¤æ–‡ä»¶ã€æ¾„æ¸…éœ€æ±‚æˆ–ä¸‹ä¸€æ­¥æ“ä½œï¼‰ï¼Œä½ å¿…é¡»ä½¿ç”¨ 'ask_user_confirmation' å·¥å…·ã€‚",
            "ä¸è¦åœ¨æ–‡æœ¬å›å¤ä¸­ç›´æ¥æé—®ã€‚æ–‡æœ¬å›å¤ä»…ç”¨äºå±•ç¤ºæ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆã€‚è¯·ä½¿ç”¨å·¥å…·æ¥è§¦å‘å¼¹å‡ºå¯¹è¯æ¡†ã€‚",
            "",
            "ç­–ç•¥ [æ€è€ƒè§„èŒƒ]:",
            "1. ä½ çš„æ€è€ƒè¿‡ç¨‹ (Reasoning) ä»…ç”¨äºåˆ†æé—®é¢˜ã€è§„åˆ’æ­¥éª¤å’Œåæ€ç»“æœã€‚",
            "2. ä¸¥ç¦å°†æœ€ç»ˆç»™ç”¨æˆ·çš„å›å¤ï¼ˆå¦‚ä»»åŠ¡æ€»ç»“ã€æ–‡ä»¶åˆ—è¡¨ã€ç»“æœæ±‡æŠ¥ï¼‰æ”¾åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ã€‚",
            "3. æ€è€ƒè¿‡ç¨‹å¯¹ç”¨æˆ·æ˜¯æŠ˜å çš„ï¼Œç”¨æˆ·ä¸»è¦é˜…è¯»çš„æ˜¯ä½ çš„æœ€ç»ˆ Content å›å¤ã€‚"
        ]
        if self.parent_agent_id:
            context_lines.append(f"Note: You are a sub-agent (ID: {self.parent_agent_id}). Perform your assigned task efficiently.")

        memories_text = ""
        if self.config_manager:
            try:
                history_dir = self.config_manager.get_chat_history_dir()
                memories_path = os.path.join(history_dir, "memories.md")
                if os.path.exists(memories_path):
                    with open(memories_path, "r", encoding="utf-8") as f:
                        memories_text = f.read().strip()
            except Exception:
                memories_text = ""
        if memories_text:
            context_lines.append("\n# Memories\n" + memories_text)

        # Append Skill-Specific Prompts (e.g. usage guidelines, learned experiences)
        if self.skill_manager.skill_prompts:
            context_lines.append("\n# Skill Capabilities & Guidelines")
            context_lines.extend(self.skill_manager.skill_prompts)

        system_prompt = "\n".join(context_lines)
        
        # Insert System Message
        current_messages.insert(0, {"role": "system", "content": system_prompt})
        
        full_reasoning = ""
        final_content = ""
        turn_count = 0
        total_duration = 0
        generated_messages = []
        
        last_tool_signature = None
        repetition_count = 0
        
        last_turn_reasoning = None
        reasoning_repetition_count = 0
        
        while True:
            # Check Control Flags
            while self.is_paused:
                if self.is_stopped: break
                self.msleep(100)
            if self.is_stopped: 
                final_content = "âš ï¸ Operation stopped by user."
                break

            turn_count += 1
            self.step_signal.emit(f"Turn {turn_count}: Requesting LLM...")

            # --- Hot Reload Skills ---
            # Check if any new skills were added or modified
            if self.skill_manager.check_for_updates():
                self.step_signal.emit("System: Detecting skill updates... Reloading.")
                self.skill_manager.load_skills()
                self.tools = self.skill_manager.get_tool_definitions()
            # -------------------------

            # Reset reasoning for the current turn (for UI display)
            current_turn_reasoning = ""

            if self.api_key:
                try:
                    start_time = time.time()
                    
                    # Create Provider via Factory
                    provider = LLMFactory.create_provider(self.config_manager)
                    stream = provider.chat_stream(current_messages, tools=self.tools)
                    
                    # Streaming Buffers
                    chunk_reasoning = ""
                    chunk_content = ""
                    tool_calls_buffer = {} # Index -> ToolCall object (dict)
                    
                    for chunk in stream:
                        # Check Pause/Stop during stream
                        while self.is_paused:
                             if self.is_stopped: break
                             self.msleep(100)
                        if self.is_stopped: break
                        
                        type_ = chunk.get("type")
                        
                        # 1. Handle Reasoning
                        if type_ == "reasoning":
                            r_content = chunk["content"]
                            current_turn_reasoning += r_content
                            full_reasoning += r_content
                            self.thinking_signal.emit(r_content)
                            
                        # 2. Handle Content
                        elif type_ == "content":
                            c_content = chunk["content"]
                            chunk_content += c_content
                            self.content_signal.emit(c_content)
                        
                        # 3. Handle Tool Calls
                        elif type_ == "tool_call":
                            index = chunk.get("index", 0) # Default to 0 if not provided
                            
                            if index not in tool_calls_buffer:
                                tool_calls_buffer[index] = {
                                    "id": chunk.get("id"),
                                    "type": "function",
                                    "function": {
                                        "name": chunk["function"].get("name", ""),
                                        "arguments": ""
                                    }
                                }
                            
                            # Append arguments
                            if "arguments" in chunk["function"]:
                                tool_calls_buffer[index]["function"]["arguments"] += chunk["function"]["arguments"]
                        
                        # 4. Handle Error
                        elif type_ == "error":
                            self.output_signal.emit(f"Provider Error: {chunk['content']}")

                    end_time = time.time()
                    duration = end_time - start_time
                    total_duration += duration
                    
                    # --- Reasoning Loop Detection ---
                    if current_turn_reasoning and len(current_turn_reasoning) > 10: # Ignore very short reasonings
                        if current_turn_reasoning == last_turn_reasoning:
                            reasoning_repetition_count += 1
                        else:
                            reasoning_repetition_count = 0
                            last_turn_reasoning = current_turn_reasoning
                            
                        if reasoning_repetition_count >= 3:
                            self.step_signal.emit("ç³»ç»Ÿ: ğŸ›‘ æ£€æµ‹åˆ°æ€ç»´æ­»å¾ªç¯ (é‡å¤çš„æ€è€ƒè¿‡ç¨‹)ã€‚è‡ªåŠ¨åœæ­¢ã€‚")
                            final_content = "âš ï¸ æ“ä½œå·²åœæ­¢: æ£€æµ‹åˆ°æ€ç»´æ­»å¾ªç¯ (é‡å¤çš„æ€è€ƒè¿‡ç¨‹)ã€‚"
                            break
                    # --------------------------------

                    # Reconstruct final message object from buffers
                    content = chunk_content
                    
                    # Reconstruct tool_calls list
                    tool_calls = []
                    if tool_calls_buffer:
                        # Convert buffer to list of objects mimicking OpenAI ToolCall
                        # We need to be careful to match the structure expected by the loop logic
                        for idx in sorted(tool_calls_buffer.keys()):
                            t_data = tool_calls_buffer[idx]
                            # Create a simple object structure
                            class ToolCallObj:
                                pass
                            class FunctionObj:
                                pass
                                
                            t_obj = ToolCallObj()
                            t_obj.id = t_data["id"]
                            t_obj.type = t_data["type"]
                            t_obj.function = FunctionObj()
                            t_obj.function.name = t_data["function"]["name"]
                            t_obj.function.arguments = t_data["function"]["arguments"]
                            
                            tool_calls.append(t_obj)

                    # Append Assistant Message to History (Manual reconstruction)
                    assistant_msg = {
                        "role": "assistant",
                        "content": content
                    }
                    # CRITICAL: For tool calls WITHIN the same turn, DeepSeek requires reasoning_content
                    # We must use current_turn_reasoning, NOT full_reasoning, to avoid duplication in history
                    # Always include the key, even if empty, to satisfy API requirements
                    assistant_msg["reasoning_content"] = current_turn_reasoning
                    # Also add 'reasoning' for UI compatibility (used by MainWindow)
                    assistant_msg["reasoning"] = current_turn_reasoning
                        
                    if tool_calls:
                         # For history, we need the dict representation
                         assistant_msg["tool_calls"] = [
                             {
                                 "id": t.id,
                                 "type": t.type,
                                 "function": {
                                     "name": t.function.name,
                                     "arguments": t.function.arguments
                                 }
                             } for t in tool_calls
                         ]
                    current_messages.append(assistant_msg)
                    generated_messages.append(assistant_msg)
                    
                    if tool_calls:
                        # --- Loop Detection ---
                        try:
                            current_signature = json.dumps(
                                sorted([{"name": t.function.name, "args": json.loads(t.function.arguments)} for t in tool_calls], key=lambda x: x['name']),
                                sort_keys=True
                            )
                            if current_signature == last_tool_signature:
                                repetition_count += 1
                            else:
                                repetition_count = 0
                                last_tool_signature = current_signature
                                
                            if repetition_count >= 3: # Same toolset called 4 times in a row
                                self.step_signal.emit("ç³»ç»Ÿ: ğŸ›‘ æ£€æµ‹åˆ°å¾ªç¯ (é‡å¤çš„å·¥å…·è°ƒç”¨)ã€‚è‡ªåŠ¨åœæ­¢ã€‚")
                                final_content = "âš ï¸ æ“ä½œå·²åœæ­¢: æ£€æµ‹åˆ°æ­»å¾ªç¯ (é‡å¤çš„å·¥å…·è°ƒç”¨)ã€‚"
                                break
                        except Exception as e:
                            print(f"Loop detection error: {e}")
                        # ----------------------

                        self.step_signal.emit(f"Tool Calls Detected: {len(tool_calls)}")
                        for tool in tool_calls:
                            # Check Control Flags inside tool loop
                            while self.is_paused:
                                if self.is_stopped: break
                                self.msleep(100)
                            if self.is_stopped: break
                            
                            name = tool.function.name
                            args = json.loads(tool.function.arguments)
                            self.step_signal.emit(f"Executing Tool: {name}({args})")
                            
                            # Emit Tool Call Signal
                            self.tool_call_signal.emit({
                                "id": tool.id,
                                "name": name,
                                "args": args
                            })
                            
                            # Report Active Skill
                            skill_name = self.skill_manager.get_skill_of_tool(name)
                            if skill_name:
                                self.skill_used_signal.emit(skill_name)
                            
                            # Execute via Skill Manager
                            # Pass step_signal as context to allow tools to log
                            result = self.skill_manager.call_tool(
                                name, 
                                args, 
                                context={
                                    "step_signal": self.step_signal, 
                                    "config_manager": self.config_manager,
                                    "skill_manager": self.skill_manager,
                                    "agent_state_signal": self.agent_state_signal,
                                    "tool_call_id": tool.id,
                                    "abort_signal": self.abort_signal
                                }
                            )
                            
                            # Emit Tool Result Signal
                            self.tool_result_signal.emit({
                                "id": tool.id,
                                "result": str(result)
                            })

                            tool_msg = {
                                "role": "tool",
                                "tool_call_id": tool.id,
                                "content": str(result) # Ensure content is string to avoid API errors
                            }
                            current_messages.append(tool_msg)
                            generated_messages.append(tool_msg)
                            self.step_signal.emit(f"Tool Result: {result}")
                        # Loop continues to let LLM see tool results
                        continue
                    else:
                        # Final Answer
                        final_content = content
                        break
                        
                except Exception as e:
                    self.finished_signal.emit({"error": str(e)})
                    return
            else:
                # --- Mock Logic / Warning for Missing API Key ---
                time.sleep(1)
                
                reasoning = "æ£€æµ‹åˆ° API Key æœªé…ç½®æˆ– OpenAI åº“ä¸å¯ç”¨ã€‚æ— æ³•è¿æ¥åˆ° DeepSeek æ¨¡å‹ã€‚"
                full_reasoning += f"\n[System]: {reasoning}"
                self.step_signal.emit(f"System: {reasoning}")
                
                final_content = (
                    "âš ï¸ **æœªé…ç½® API Key**\n\n"
                    "è¯·ç‚¹å‡»å³ä¸Šè§’çš„ **âš™ï¸ è®¾ç½®** æŒ‰é’®é…ç½®æ‚¨çš„ DeepSeek API Keyã€‚\n"
                    "é…ç½®å®Œæˆåï¼Œæˆ‘å°†èƒ½å¤Ÿä¸ºæ‚¨æ‰§è¡Œå¤æ‚çš„æ–‡ä»¶æ“ä½œå’Œä»£ç ç”Ÿæˆä»»åŠ¡ã€‚"
                )
                
                break

        self.finished_signal.emit({
            "reasoning": full_reasoning.strip(),
            "content": final_content,
            "role": "assistant",
            "duration": total_duration,
            "generated_messages": generated_messages
        })

        self.agent_state_signal.emit({
            "agent_id": self.parent_agent_id or "Main", 
            "status": "completed", 
            "content": final_content
        })
